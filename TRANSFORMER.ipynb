{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "max_seq_length = 100\n",
        "total_word_num = 100\n"
      ],
      "metadata": {
        "id": "IUm2Qv8m16_y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eIBpxTjt1Zh0"
      },
      "outputs": [],
      "source": [
        "#MultiHeadAttention class\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_num=512, num_head=8):\n",
        "        super().__init__()\n",
        "        self.num_head = num_head\n",
        "        self.dim_num = dim_num\n",
        "\n",
        "        self.query_embed = nn.Linear(dim_num, dim_num)\n",
        "        self.key_embed = nn.Linear(dim_num, dim_num)\n",
        "        self.value_embed = nn.Linear(dim_num, dim_num)\n",
        "        self.output_embed = nn.Linear(dim_num, dim_num)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "\n",
        "      d_k = k.size()[-1]\n",
        "      k_transpose = torch.transpose(k, 2, 3)\n",
        "\n",
        "      output = torch.matmul(q, k_transpose)\n",
        "      output = output / math.sqrt(d_k)\n",
        "\n",
        "      if mask is not None:\n",
        "        output = output.masked_fill(mask.unsqueeze(1).unsqueeze(-1), 0)\n",
        "\n",
        "      output = F.softmax(output)\n",
        "      output = torch.matmul(output, v)\n",
        "\n",
        "      return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "      batch_size = q.size()[0]\n",
        "\n",
        "      #(dim_num,dim_num) -> (batch_size, -1, self.num_head, self.dim_num//self.num_head).transpose(1,2) 로 reshape#\n",
        "      q = self.query_embed(q).view(batch_size, -1, self.num_head, self.dim_num//self.num_head).transpose(1,2)\n",
        "      k = self.key_embed(k).view(batch_size, -1, self.num_head, self.dim_num // self.num_head).transpose(1,2)\n",
        "      v = self.value_embed(v).view(batch_size, -1, self.num_head, self.dim_num // self.num_head).transpose(1,2)\n",
        "\n",
        "\n",
        "      #이부분 이해가안되미어ㅣㅏ머림\n",
        "      output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "      batch_num, num_head, seq_num, hidden_num = output.size()\n",
        "      output = torch.transpose(output, 1, 2).contiguous().view((batch_size, -1, hidden_num * self.num_head))\n",
        "\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AddLayerNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "  def layer_norm(self, input):\n",
        "      mean = torch.mean(input, dim=-1, keepdim=True)\n",
        "      std = torch.std(input, dim=-1, keepdim=True)\n",
        "      output = (input-mean) / std\n",
        "\n",
        "      return output\n",
        "\n",
        "  def forward(self, input, residual):\n",
        "      #헷갈렸던 점 정리\n",
        "      #input값이 FFN을 거쳐 들어와야하는 것 아닌가?\n",
        "      # -> input값 자체가 이미 FFN을 거친 값\n",
        "      return residual + self.layer_norm(input)"
      ],
      "metadata": {
        "id": "xc2hEU9ZMKX9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim_num=512):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(dim_num, 4*dim_num)\n",
        "    self.layer2 = nn.Linear(4*dim_num, dim_num)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.layer1(input)\n",
        "    output = self.layer2(F.relu(output))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "1IlHiG368oD4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, dim_num=512):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multihead = MultiHeadAttention(dim_num = dim_num)\n",
        "    self.residual_layer1 = AddLayerNorm()\n",
        "    self.FFN = FeedForward(dim_num=dim_num)\n",
        "    self.residual_layer2 = AddLayerNorm()\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    multihead_output = self.multihead(q, k, v)\n",
        "    residual1_output = self.residual_layer1(multihead_output, q)\n",
        "    feedforward_output = self.FFN(residual1_output)\n",
        "    output = self.residual_layer2(feedforward_output, residual1_output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "CO_9Ip7Q-FN1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, dim_num=512):\n",
        "    super().__init__()\n",
        "\n",
        "    self.masked_multihead = MultiHeadAttention(dim_num = dim_num)\n",
        "    self.residual_layer1 = AddLayerNorm()\n",
        "    self.multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "    self.residual_layer2 = AddLayerNorm()\n",
        "    self.FFN = FeedForward(dim_num=dim_num)\n",
        "    self.residual_layer3 = AddLayerNorm\n",
        "\n",
        "  def forward(self, o_q, o_k, o_v, encoder_output, mask):\n",
        "\n",
        "    masked_multihead_output = self.masked_multihead(o_q, o_k, o_v, mask)\n",
        "    residual1_output = self.residual_layer1(masked_multihead_output, o_q)\n",
        "\n",
        "    # 왜 q,k자리에 encoder의 output이 들어가는걸까...............................\n",
        "    multihead_output = self.multihead(encoder_output, encoder_output, residual1_output, mask)\n",
        "    residual2_output = self.residual_layer2(multihead_output, residual1_output)\n",
        "    FFNoutput = self.FFN(residual2_output)\n",
        "    output = self.residual_layer3(FFNoutput, residual2_output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "qHOWBK2l-ja9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder_num=6, decoder_num=6, hidden_dim=512, max_encoder_seq_length=100,\n",
        "                 max_decoder_seq_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_num = encoder_num\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_encoder_seq_length = max_encoder_seq_length\n",
        "        self.max_decoder_seq_length = max_decoder_seq_length\n",
        "\n",
        "        self.input_data_embed = nn.Embedding(total_word_num, self.hidden_dim)\n",
        "        self.Encoders = [Encoder(dim_num=hidden_dim) for _ in range(encoder_num)]\n",
        "\n",
        "        self.output_data_embed = nn.Embedding(total_word_num, self.hidden_dim)\n",
        "        self.Decoders = [Decoder(dim_num=hidden_dim) for _ in range(decoder_num)]\n",
        "\n",
        "        self.last_linear_layer = nn.Linear(self.hidden_dim, max_seq_length)\n",
        "\n",
        "    def position_encoding(self, position_max_length=100):\n",
        "        position = torch.arange(0, position_max_length, dtype=torch.float).unsqueeze(1)\n",
        "        pe = torch.zeros(position_max_length, self.hidden_dim)\n",
        "        div_term = torch.pow(torch.ones(self.hidden_dim // 2).fill_(10000),\n",
        "                             torch.arange(0, self.hidden_dim, 2) / torch.tensor(self.hidden_dim, dtype=torch.float32))\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        return pe\n",
        "\n",
        "    def forward(self, input, output, mask):\n",
        "\n",
        "        #input embedding\n",
        "        input_embed = self.input_data_embed(input)\n",
        "        # + positional embedding\n",
        "        input_embed += self.position_encoding(self.max_encoder_seq_length)\n",
        "        q, k, v = input_embed, input_embed, input_embed\n",
        "\n",
        "        for encoder in self.Encoders:\n",
        "            encoder_output = encoder(q, k, v)\n",
        "            q = encoder_output\n",
        "            k = encoder_output\n",
        "            v = encoder_output\n",
        "\n",
        "        output_embed = self.output_data_embed(output)\n",
        "        output_embed += self.position_encoding(self.max_decoder_seq_length)\n",
        "        output_embed = output_embed.masked_fill(mask.unsqueeze(-1), 0)\n",
        "        d_q, d_k, d_v = output_embed, output_embed, output_embed\n",
        "\n",
        "        for decoder in self.Decoders:\n",
        "            decoder_output = decoder(d_q, d_k, d_v, encoder_output, mask)\n",
        "            d_q = decoder_output\n",
        "            d_k = decoder_output\n",
        "            d_v = decoder_output\n",
        "\n",
        "        output = F.softmax(self.last_linear_layer(decoder_output), dim=-1)\n",
        "        return output\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = Transformer()\n",
        "\n",
        "    input = torch.randint(low=0, high=max_seq_length, size=(64, max_seq_length), dtype=torch.long)\n",
        "    output = torch.randint(low=0, high=max_seq_length, size=(64, max_seq_length), dtype=torch.long)\n",
        "    mask = torch.zeros((64, max_seq_length), dtype = torch.bool)\n",
        "    mask[:, :30] = 1\n",
        "\n",
        "    output = model(input, output, mask)\n",
        "    _, pred = torch.max(output, dim=-1)\n",
        "    print(pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4845aJuHtQd",
        "outputId": "b92c7420-ecbe-48cd-8ad6-bce27684970b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5iYfhz7pJ5Zv"
      }
    }
  ]
}